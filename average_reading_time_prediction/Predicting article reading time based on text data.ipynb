{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting article reading time based on text\n",
    "This jupyter notebook attempts to predict article reading time based on article text data. \n",
    "## Prerequisites\n",
    "You will need to following:\n",
    "* a virtual environment set up with all necessary required packages as discribed in requirements.txt\n",
    "* a csv file with your article text data\n",
    "* a csv file referencing the article text data, with reading time for every article\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration and loading in the data\n",
    "\n",
    "### Configurations\n",
    "\n",
    "These are the necessary configurations:\n",
    "* DATA_DIR: the folder in which your data csv ares stored\n",
    "* DEFAULT_LANGUAGE: the default language for your articles.\n",
    "* TITLE_WEIGHT: the title is generaly more important than the full text. you can give it a higher weight here.\n",
    "    * MAX_WORD_FEATURES: number of words in the Bag Of Words representation\n",
    "* MAX_FILTERED_FEATURES: number of words after filtering\n",
    "* LSA_FEATURES: the number of latent topics after LSA\n",
    "\n",
    "### Data loading\n",
    "\n",
    "Your article text csv (article_content.csv in the example) file should look like this:\n",
    "\n",
    "| article_reference |  article_title   | article_text  |\n",
    "|-------------------|------------------|---------------|\n",
    "| article_00000001  | My first article | test test abc |\n",
    "\n",
    "Your article reading time csv (article_reading_time.csv in the example) file should look like this:\n",
    "\n",
    "| article_reference | avg_reading_time |\n",
    "|-------------------|------------------|\n",
    "| article_00000001  |        3.6       |\n",
    "\n",
    "Here, the average reading time is given in seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################\n",
    "# Configurations                                                                                       #\n",
    "########################################################################################################\n",
    "import os\n",
    "import pandas as pd\n",
    "DATA_DIR = os.path.join(\".\",\"data\")\n",
    "DEFAULT_LANGUAGE = 'english'\n",
    "TITLE_WEIGHT = 3\n",
    "MAX_WORD_FEATURES=20000\n",
    "MAX_FILTERED_FEATURES=7500\n",
    "LSA_FEATURES=500\n",
    "########################################################################################################\n",
    "# loading in the data                                                                                  #\n",
    "########################################################################################################\n",
    "article_content_df = pd.read_csv(os.path.join(DATA_DIR,\"article_content.csv\"))\n",
    "article_reading_time_df = pd.read_csv(os.path.join(DATA_DIR,\"article_reading_time.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if your loaded data looks ok as in the example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_content_df.set_index(\"article_reference\",inplace=True)\n",
    "article_content_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_reading_time_df.set_index(\"article_reference\",inplace=True)\n",
    "article_reading_time_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "### Stemming the words\n",
    "\n",
    "To avoid variations in the words, we need to stem the words. There are multiple libraries such as the snowballstemmer for this. (Also, if you are working with data in English, lemmatization might be prefered to stemming). But afterwards we want to be able to get the actual word used, and not the stemmed word. Thus, we provide a wrapper around the SnowballStemmer that keeps a memory of the words it stemmed. This will cause some overhead if you are working with a lot of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from statistics import mode \n",
    "\n",
    "class ReversibleSnowBallStemmer(nltk.stem.SnowballStemmer):\n",
    "    \"\"\" A wrapper around snowball stemmer with a reverse lookip table \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(self.__class__, self).__init__(*args, **kwargs)\n",
    "        self._stem_memory = nltk.defaultdict(list)\n",
    "        # switch stem and memstem\n",
    "        self._stem = self.stem\n",
    "        self.stem = self.memstem\n",
    "\n",
    "    def memstem(self, word):\n",
    "        \"\"\" Wrapper around stem that remembers \"\"\"\n",
    "        stemmed_word = self._stem(word)\n",
    "        self._stem_memory[stemmed_word].append(word)\n",
    "        return stemmed_word\n",
    "\n",
    "    def unstem(self, stemmed_word):\n",
    "        \"\"\" Reverse lookup \"\"\"\n",
    "        return mode(self._stem_memory[stemmed_word])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the full text\n",
    "\n",
    "We now create the full text by weighing the title with the title weight and adding the rest of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_content_df[\"fulltext\"] = TITLE_WEIGHT * (article_content_df[\"article_title\"] + \" \") \\\n",
    "+ article_content_df[\"article_text\"]\n",
    "#check if the first article looks ok\n",
    "article_content_df[\"fulltext\"].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## splitting in words and deleting stopwords\n",
    "\n",
    "We still need to split the text in words, delete stopwords and apply our reversible stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = ReversibleSnowBallStemmer(DEFAULT_LANGUAGE)\n",
    "from nltk.corpus import stopwords\n",
    "import re \n",
    "\n",
    "def text_to_words(text):\n",
    "    # remove punctuation, numbers\n",
    "    try:\n",
    "        text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    # the csv file with the html dataframe removes unicode, so the text can be NaN\n",
    "    except Exception as e:\n",
    "        print(f\"Exception while processing text {text}: {e}\")\n",
    "        text = \"\"\n",
    "    # split in words\n",
    "    words = text.lower().split()\n",
    "    # remove stop words\n",
    "    stops = set(stopwords.words(DEFAULT_LANGUAGE))\n",
    "    filtered = [w for w in words if w not in stops]\n",
    "    # stem\n",
    "    stemmed = [stemmer.stem(w) for w in filtered]\n",
    "    return \" \".join(stemmed)\n",
    "article_content_df[\"preprocessed\"] = article_content_df[\"fulltext\"].apply(text_to_words)\n",
    "article_content_df[\"preprocessed\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "article_content_and_reading_time_df = article_content_df.merge(\n",
    "    article_reading_time_df,\n",
    "    left_index=True,\n",
    "    right_index=True\n",
    ")\n",
    "article_content_and_reading_time_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorizing the data and splitting the data in train and test data\n",
    "\n",
    "### Categorizing the data\n",
    "\n",
    "We want to build a classification algorithm and not a regression algorithm, therefore we make discrete categories of average reading time. This implementation just uses the quantiles of the reading time. The lowest 25% are classified as `LOW`, the highest 25% is classified as `HIGH`, the quantiles in between are respectively `LOWER THAN AVERAGE` and `HIGHER THAN AVERAGE`\n",
    "\n",
    "### Splitting the data in train and test data\n",
    "\n",
    "We want to be able to test if the model is actually learning. Thus, we need a separate batch of train data and a separate batch of test data. We will use 75% of the data to train on and 25% to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "quantile_25 =  article_content_and_reading_time_df[\"avg_reading_time\"].quantile(0.25)\n",
    "quantile_50 =  article_content_and_reading_time_df[\"avg_reading_time\"].quantile(0.5)\n",
    "quantile_75 =  article_content_and_reading_time_df[\"avg_reading_time\"].quantile(0.75)\n",
    "\n",
    "article_content_and_reading_time_df[\"avg_reading_time_category\"] = \"NONE\"\n",
    "article_content_and_reading_time_df.loc[\n",
    "    article_content_and_reading_time_df[\"avg_reading_time\"] < quantile_25,\n",
    "    \"avg_reading_time_category\"\n",
    "] = \"LOW\"\n",
    "article_content_and_reading_time_df.loc[\n",
    "    (\n",
    "    (article_content_and_reading_time_df[\"avg_reading_time\"] >= quantile_25)\n",
    "    &\n",
    "    (article_content_and_reading_time_df[\"avg_reading_time\"] < quantile_50)\n",
    "    ),\n",
    "    \"avg_reading_time_category\"\n",
    "] = \"LOWER THAN AVERAGE\"\n",
    "article_content_and_reading_time_df.loc[\n",
    "    (article_content_and_reading_time_df[\"avg_reading_time\"] >= quantile_50)\n",
    "    &\n",
    "    (article_content_and_reading_time_df[\"avg_reading_time\"] < quantile_75),\n",
    "    \"avg_reading_time_category\"\n",
    "] = \"HIGHER THAN AVERAGE\"\n",
    "article_content_and_reading_time_df.loc[\n",
    "    article_content_and_reading_time_df[\"avg_reading_time\"] >= quantile_75,\n",
    "    \"avg_reading_time_category\"\n",
    "] = \"HIGH\"\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    article_content_and_reading_time_df[\"preprocessed\"],\n",
    "    article_content_and_reading_time_df[\"avg_reading_time_category\"]\n",
    ")\n",
    "article_content_and_reading_time_df.groupby(\"avg_reading_time_category\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the model\n",
    "\n",
    "Applying the model consits of the following steps:\n",
    "\n",
    "* convert the preprocessed text to a bag of words representation (Countvectorizer)\n",
    "* filter the bag of words for the most relevant terms (ffilter)\n",
    "* apply term frequency inverse document frequency as a word relevancy metric (tfidftransformer)\n",
    "* build a latent topic model by applying the LSA algorithm (svdAlgorithm)\n",
    "* normalize the features (Normalizer)\n",
    "* apply the support vector machine (clf)\n",
    "\n",
    "There are 2 steps:\n",
    "\n",
    "* fitting the model on train data\n",
    "* appling the model to test data\n",
    "\n",
    "You can see them in the next 2 cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import svm\n",
    "\n",
    "countVectorizer = CountVectorizer(analyzer=\"word\", tokenizer=None,\n",
    "                                               preprocessor=None,\n",
    "                                               stop_words=None,\n",
    "                                               max_features=MAX_WORD_FEATURES)\n",
    "tfidfTransformer = TfidfTransformer()\n",
    "ffilter = SelectKBest(chi2, k=MAX_FILTERED_FEATURES)\n",
    "svd_algorithm = TruncatedSVD(LSA_FEATURES)\n",
    "normalizer = Normalizer()\n",
    "clf = OneVsRestClassifier(svm.LinearSVC(class_weight='balanced'))\n",
    "\n",
    "bow = countVectorizer.fit_transform(X_train, Y_train)\n",
    "bow_filtered = ffilter.fit_transform(bow, Y_train.values)\n",
    "tfidf = tfidfTransformer.fit_transform(bow_filtered, Y_train.values)\n",
    "svd = svd_algorithm.fit_transform(tfidf, Y_train.values)\n",
    "features = normalizer.fit_transform(svd, Y_train.values)\n",
    "clf.fit(features, Y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = countVectorizer.transform(X_test)\n",
    "bow_filtered = ffilter.transform(bow)\n",
    "tfidf = tfidfTransformer.transform(bow_filtered)\n",
    "svd = svd_algorithm.transform(tfidf)\n",
    "features = normalizer.transform(svd)\n",
    "predicted = clf.predict(features)\n",
    "predicted_df = pd.DataFrame(data={'PREDICTED_CATEGORY': predicted},\n",
    "                            index=X_test.index)\n",
    "predicted_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model on test data\n",
    "\n",
    "Before checking what the model learned, we need to test if the model actually learned something. We check 2 things in the following cells:\n",
    "\n",
    "* We check the Precision, Recall, F1-score and Support metric for the model. See the links for a more detailed explanation of what these metrics mean and why they matter\n",
    "* We check the confusion matrix. The darker your diagonal from the top left to the bottom right, the better your model. The matrix shows us how many times the true label matches the label predicted by the model and how many times it makes certain mistakes (e.g. how many times does it predict a low average reading time when the true average reading time is high? ). Some mistakes are worse than others. E.g. in this case, mistaking `LOWER THAN AVERAGE` and `HIGHER THAN AVERAGE` would not be that bad, while mistaking `HIGH` for `LOW` would be a lot worse. We can see how many times each kind of mistake is made, and try to think what that would mean for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "precision, recall, fscore, support = score(Y_test,\n",
    "                                           predicted_df,\n",
    "                                           labels=label_order)\n",
    "print(\"Categories: {}\".format(label_order))\n",
    "print('precision: {}'.format(precision))\n",
    "print('recall: {}'.format(recall))\n",
    "print('fscore: {}'.format(fscore))\n",
    "print('support: {}'.format(support))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix \n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "def create_confusion_matrix(true_labels, predicted_labels, labels):\n",
    "    cm = confusion_matrix(true_labels, predicted_labels, labels=labels)\n",
    "    cm_df = pd.DataFrame(data=cm, index=labels, columns=labels)\n",
    "    cm_df.index.name = \"True label\"\n",
    "    cm_df.columns.name = \"Predicted label\"\n",
    "    return cm_df\n",
    "\n",
    "def plot_confusion_matrix(cm_df, normalize=False, title='Confusion matrix'):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm_df = cm_df.div(cm_df.sum(axis=1), axis=0)\n",
    "    cmap = plt.cm.Blues\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(cm_df, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(cm_df.columns))\n",
    "    plt.xticks(tick_marks, cm_df.columns, rotation=90)\n",
    "    plt.yticks(tick_marks, cm_df.index)\n",
    "\n",
    "    thresh = cm_df.max().max() / 2.\n",
    "    for true, pred in itertools.product(range(cm_df.shape[0]),\n",
    "                                        range(cm_df.shape[1])):\n",
    "        value = np.round(cm_df.iat[true, pred], 2)\n",
    "        plt.text(pred, true, value,\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if (value > thresh) else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel(cm_df.index.name)\n",
    "    plt.xlabel(cm_df.columns.name)\n",
    "    plt.show()\n",
    "\n",
    "label_order = [\"HIGH\",\"HIGHER THAN AVERAGE\",\"LOWER THAN AVERAGE\",\"LOW\"]\n",
    "cm = create_confusion_matrix(Y_test, predicted_df,\n",
    "                                        label_order)\n",
    "plot_confusion_matrix(cm, normalize=True, title=\"Confusion matrix\")\n",
    "plot_confusion_matrix(cm, normalize=False, title=\"Confusion matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What did the model learn?\n",
    "\n",
    "Now we can investigate what the model learned. Which words are the most important words to predict a `HIGH` average reading time and which are the most important to predict a `LOW` reading time? We have to trace our way back through the whole model for investigation. First we investigate the weights of the model for these categories. further we backtrace the weights to the topic model, and multiply the weight for that topic found in the support vector machine by the weight of the words in the bow representation for that topic. By summing all those multiplications for each word, we find the importance of that word\n",
    "\n",
    "After that, we create a wordcloud from it, so we can visualize the importance of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_names():\n",
    "    return np.asarray(countVectorizer.get_feature_names())[\n",
    "        ffilter.get_support()]\n",
    "\n",
    "def get_top_terms( n=250):\n",
    "    labels = clf.classes_\n",
    "    #get the weights for features from the support vectore machine\n",
    "    feature_weights = [est.coef_[0] for est in clf.estimators_]\n",
    "    feature_weights = np.array(feature_weights)\n",
    "    #Get all words in the bow representation after filtering\n",
    "    terms = get_feature_names()\n",
    "    #Get the importance of all words (sum of multiplications as specified above) from the model weights and the\n",
    "    #latent topic model weight\n",
    "    original_space_features = svd_algorithm.inverse_transform(\n",
    "        feature_weights)\n",
    "    # get the indexes of the words, ranked by importance of the word\n",
    "    order_features = original_space_features.argsort()[:, ::-1]\n",
    "    #For every predicted label\n",
    "    for i in range(len(labels)):\n",
    "        text = \"\"\n",
    "        for ind in order_features[i,:n]:\n",
    "            #we create a text in with the word multiplied by its importance for the wordcloud representation later\n",
    "            for _ in range(int(round(original_space_features[i,ind] * 25))):\n",
    "                #unstem the words\n",
    "                text += stemmer.unstem(terms[ind])\n",
    "                text += \" \"\n",
    "        yield (labels[i],text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "from PIL import Image\n",
    "mask = np.array(Image.open(os.path.join(DATA_DIR,\"blue-cloud-hi.png\")))\n",
    "for label, text in get_top_terms():\n",
    "    if label in (\"HIGH\",\"LOW\"):\n",
    "        print(label)\n",
    "        wc = WordCloud(width=1000,height=500,background_color='white',collocations=False,max_words=250, mask=mask, margin=10,\n",
    "           random_state=1).generate(text)\n",
    "        # store default colored image\n",
    "        default_colors = wc.to_array()\n",
    "        wc.to_file(\"low.png\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(default_colors, interpolation=\"bilinear\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "That should be it. Your wordcloud should be there in the image above. If there are words in there that seem weird or not understandable, you can investigate and see in which articles these words were actually used. \n",
    "\n",
    "If you have problems with repeating the work in this notebook, don't hesitate to contact me at engineering@twipemobile.com or create an issue in the git."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
